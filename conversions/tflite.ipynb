{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to TFlite Model"
      ],
      "metadata": {
        "id": "6lFjkpu_A4t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import os\n",
        "\n",
        "MODEL_DIR       = \"/content/drive/MyDrive/Colab Notebooks/Actionable-Fine-Tune/mobilebert-finetuned-actionable-v2\"\n",
        "MAX_LEN         = 128                    # sequence length used during fine-tuning\n",
        "SAVEDMODEL_DIR  = \"mobilebert_savedmodel_f32\"\n",
        "TFLITE_OUT      = \"mobilebert_float32.tflite\"\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1Ô∏è‚É£  LOAD MODEL & TOKENIZER (PyTorch ‚Üí TF)\n",
        "# -----------------------------------------------------------\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    from_pt=True                        # convert safetensors ‚Üí TensorFlow\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2Ô∏è‚É£  EXPORT AS SAVEDMODEL WITH CORRECT SIGNATURE\n",
        "# -----------------------------------------------------------\n",
        "if os.path.exists(SAVEDMODEL_DIR):\n",
        "    !rm -rf $SAVEDMODEL_DIR\n",
        "\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"input_ids\"),\n",
        "    tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"attention_mask\"),\n",
        "    tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"token_type_ids\"),\n",
        "])\n",
        "def serving_fn(input_ids, attention_mask, token_type_ids):\n",
        "    return model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "\n",
        "tf.saved_model.save(\n",
        "    model,\n",
        "    SAVEDMODEL_DIR,\n",
        "    signatures={\"serving_default\": serving_fn},\n",
        ")\n",
        "print(\"‚úÖ SavedModel exported ‚Üí\", SAVEDMODEL_DIR)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3Ô∏è‚É£  CONVERT TO FLOAT-32 TFLITE  (no quantisation)\n",
        "# -----------------------------------------------------------\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVEDMODEL_DIR)\n",
        "# No optimisations / quant flags ‚Üí pure float32 graph\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(TFLITE_OUT, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"üéâ  Float32 TFLite model written ‚Üí  {TFLITE_OUT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcWVvQc_MK_G",
        "outputId": "38c9dead-9a2c-428d-ce66-f0b38b92c8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFMobileBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFMobileBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SavedModel exported ‚Üí mobilebert_savedmodel_f32\n",
            "üéâ  Float32 TFLite model written ‚Üí  mobilebert_float32.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYLqjYGuWPYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8LhADp4WPaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pgj9BlI2WPcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdbzC2-JWPe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VERHUaQxWPg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKA_QkHhWPjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoRTKHWpWPmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Intefence"
      ],
      "metadata": {
        "id": "Vr94qC5WMNlz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbUqudo9VVP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 0.  Imports & paths\n",
        "# ---------------------------------------------------------\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import scipy.special, re\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_DIR       = \"/content/drive/MyDrive/Colab Notebooks/Actionable-Fine-Tune/mobilebert-finetuned-actionable-v2\"\n",
        "TFLITE_PATH     = \"mobilebert_float32.tflite\"\n",
        "CSV_PATH        = \"data.csv\"  # must contain 'text' and 'label'\n",
        "MAX_LEN         = 128\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1.  Load tokenizer\n",
        "# ---------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2.  Load TFLite model and resize inputs\n",
        "# ---------------------------------------------------------\n",
        "interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "for inp in interpreter.get_input_details():\n",
        "    interpreter.resize_tensor_input(inp[\"index\"], [1, MAX_LEN])\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details  = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "def _base_key(tflite_name: str) -> str:\n",
        "    return re.sub(r\"^serving_default_|:\\d+$\", \"\", tflite_name)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3.  Inference function\n",
        "# ---------------------------------------------------------\n",
        "def predict(text: str):\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"np\",\n",
        "    )\n",
        "\n",
        "    for inp in input_details:\n",
        "        idx   = inp[\"index\"]\n",
        "        dtype = inp[\"dtype\"]\n",
        "        key   = _base_key(inp[\"name\"])\n",
        "        tensor = enc[key]\n",
        "\n",
        "        if dtype in (np.int32, np.int64):\n",
        "            interpreter.set_tensor(idx, tensor.astype(dtype))\n",
        "        else:\n",
        "            scale, zp = inp[\"quantization\"]\n",
        "            q = np.round(tensor.astype(np.float32) / scale + zp).astype(dtype)\n",
        "            interpreter.set_tensor(idx, q)\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    out_info = output_details[0]\n",
        "    raw = interpreter.get_tensor(out_info[\"index\"])\n",
        "    if out_info[\"dtype\"] in (np.int8, np.uint8):\n",
        "        scale, zp = out_info[\"quantization\"]\n",
        "        logits = (raw.astype(np.float32) - zp) * scale\n",
        "    else:\n",
        "        logits = raw.astype(np.float32)\n",
        "\n",
        "    probs = scipy.special.softmax(logits, axis=-1)[0]\n",
        "    pred  = int(np.argmax(probs))\n",
        "    return pred, probs\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4.  Load dataset and evaluate accuracy\n",
        "# ---------------------------------------------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "texts  = df[\"text\"].astype(str).tolist()\n",
        "labels = df[\"label\"].astype(int).tolist()\n",
        "\n",
        "correct = 0\n",
        "total   = 0\n",
        "mistakes = []\n",
        "\n",
        "for text, label in tqdm(zip(texts, labels), total=len(texts)):\n",
        "    pred, _ = predict(text)\n",
        "    if pred == label:\n",
        "        correct += 1\n",
        "    else:\n",
        "        mistakes.append((text, label, pred))\n",
        "    total += 1\n",
        "\n",
        "acc = correct / total\n",
        "print(f\"\\n‚úÖ Accuracy on {total} examples: {acc * 100:.2f}%\")\n",
        "\n",
        "# Optional: Show some mistakes\n",
        "print(\"\\n‚ùå Sample misclassifications:\")\n",
        "for i in range(min(5, len(mistakes))):\n",
        "    t, true, pred = mistakes[i]\n",
        "    print(f\"  ‚Ä¢ '{t[:60]}...' ‚Üí true={true}, pred={pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dSDk-8_VVTm",
        "outputId": "44dedf85-3807-42a3-ad69-8df2b8194d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1301/1301 [02:59<00:00,  7.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Accuracy on 1301 examples: 96.00%\n",
            "\n",
            "‚ùå Sample misclassifications:\n",
            "  ‚Ä¢ 'What‚Äôs playing at the cinema?...' ‚Üí true=1, pred=0\n",
            "  ‚Ä¢ 'How many steps have I taken today?...' ‚Üí true=1, pred=0\n",
            "  ‚Ä¢ 'Where‚Äôs the nearest gas station?...' ‚Üí true=1, pred=0\n",
            "  ‚Ä¢ 'What time does the show start?...' ‚Üí true=0, pred=1\n",
            "  ‚Ä¢ 'Is it windy outside?...' ‚Üí true=1, pred=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJ88O7eHVXBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}